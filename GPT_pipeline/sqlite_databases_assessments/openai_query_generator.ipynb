{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25d6f12-00d5-4e59-8dcd-57d2291fc154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from openai import AzureOpenAI\n",
    "import openai\n",
    "import os\n",
    "\n",
    "current_directory = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "os.chdir(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2df303a-b511-4fe5-864c-9345c00ef6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_version=\"2023-05-15\",\n",
    "    azure_endpoint=\"https://..\", # replace with azure endpoint\n",
    "    api_key=\"...\", # replace with your API key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d6f1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input file names: those are replaced each time based on assessment and SQL database type.\n",
    "prompt_file=\"./../prompt.md\" # define prompt file to use\n",
    "metadata_file=\"./../metadata.sql\" # define metadata file to use\n",
    "output_queries = '' # spacify output file name for AI-generated SQL queries\n",
    "\n",
    "# for example for postgreSQL first assessment: prompt_file =\"./postgres_assessment/post_prompt_baseline.md\", metadata_file=\"./postgres_assessment/post_metadata_baseline.sql\"\n",
    "# output file = \"./postgres_assessment/postgres_1_queries.sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61cba434-20b7-4116-a9b0-4464b53566ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_prompt(question, prompt_file, metadata_file):\n",
    "    \"\"\"\n",
    "    Generates a customized prompt by replacing placeholders in a template with actual data.\n",
    "\n",
    "    Args:\n",
    "        question (str): The user's question to be included in the prompt.\n",
    "        prompt_file (str): Path to a file containing a template prompt with placeholders.\n",
    "        metadata_file (str): Path to a file containing metadata (e.g., database table information).\n",
    "\n",
    "    Returns:\n",
    "        str: The generated prompt with placeholders replaced by actual data.\n",
    "    \"\"\"\n",
    "    with open(prompt_file, \"r\") as f:\n",
    "        prompt = f.read()\n",
    "    \n",
    "    with open(metadata_file, \"r\") as f:\n",
    "        table_metadata_string = f.read()\n",
    "        \n",
    "\n",
    "    prompt = prompt.format(\n",
    "        user_question=question, table_metadata_string=table_metadata_string)\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce07e8d4-62f7-4273-a076-9fa2211f3ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SQL_query_generation(question, prompt_file, metadata_file):\n",
    "    \"\"\"\n",
    "    Generates an SQL query response based on a user question and template prompts.\n",
    "\n",
    "    Args:\n",
    "        question (str): The user's question or query.\n",
    "        prompt_file (str): Path to a file containing template prompts for OpenAI.\n",
    "        metadata_file (str): Path to a file containing relevant metadata (e.g., table information).\n",
    "        temp (float): Temperature parameter for controlling response randomness.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated SQL query response.\n",
    "    \"\"\"\n",
    "    prompt = generate_prompt(question, prompt_file , metadata_file)\n",
    "    try:\n",
    "        sys_prompt = prompt.split(\"### Input:\")[0]\n",
    "        user_prompt = prompt.split(\"### Input:\")[1].split(\"### Response:\")[0]\n",
    "        assistant_prompt = prompt.split(\"### Response:\")[1]\n",
    "    except:\n",
    "        raise ValueError(\"Invalid prompt file. Please use prompt_openai.md\")\n",
    "  \n",
    "    messages = []\n",
    "    messages.append({\"role\": \"system\", \"content\": sys_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": assistant_prompt})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"chat\",\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "    return response.model_dump()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "099d65d6-5aa7-4623-b509-f210ff41b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../questions.txt', 'r') as f:\n",
    "    questions = f.read().split('\\n')\n",
    "\n",
    "with open(output_queries, \"w\") as file:\n",
    "    for i,question in enumerate(questions):\n",
    "        file.write(f\"--{i+1} {question}\\n\")\n",
    "        SQLcode = SQL_query_generation(question, prompt_file , metadata_file).split(\"```\")\n",
    "        #SQLcode = value.split(\"```\")\n",
    "        SQLcode_to_save = SQLcode[0].strip()\n",
    "        if SQLcode_to_save[-1] == ';':\n",
    "            file.write(f\"{SQLcode_to_save}\\n\\n\")\n",
    "        else:\n",
    "            file.write(f\"{SQLcode_to_save};\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
